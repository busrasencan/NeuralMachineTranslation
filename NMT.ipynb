{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1pSqLcGm_MclRJTH6qBwDlFiDRCIZx_2d",
      "authorship_tag": "ABX9TyOW+7nfGe9pLTTxq6aGAp4x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/busrasencan/NeuralMachineTranslation/blob/master/NMT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c1IPrbKLSoo",
        "colab_type": "text"
      },
      "source": [
        "# **NMT**\n",
        "\n",
        "NMT, doğal dil işlemedeki en önemli alanlardan birisidir.\n",
        "\n",
        "NMT'nin iki parçaya ihtiyacı olduğu için iyi bir test aracıdır.\n",
        "- Dil analizi ve dil modelleme.\n",
        "\n",
        "Günlük hayatta, ticartte, diplomaside, askeride makine çevirisine çok büyük ihtiyaç var.\n",
        "\n",
        "- Yıllık 40 milyar dolarlık endüstri.\n",
        "\n",
        "Google 2016'da Google Translate'te makine çevirisine geçti. Günlük 100 milyardan fazla kelime çeviriyor.\n",
        "\n",
        "Facebook 2016'da kendi makine çevirisi modelini geliştirdi.\n",
        "\n",
        "Ebay sınır ötesi ticaret için makine çevirisine bağımlı.\n",
        "\n",
        "Çeviri yapabilmek için uçtan uca eğitebileceğimiz büyükçe tek bir yapay sinir ağı oluşturmaya neural machine translation denir.\n",
        "\n",
        "Bu çalışmada seq2seq kullanacağız. Hadi başlayalım.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97TGXvwhLOxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install tensorflow==2.0.0-rc0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dL9Hd54zFEro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra_i-OLnFg7r",
        "colab_type": "code",
        "outputId": "6c550d4b-adb2-48a8-d5f7-c0f29d46c137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0-rc0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5I25ICjOF6QP",
        "colab_type": "code",
        "outputId": "8386addd-0298-4fcc-ea44-922328ac7d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tf.keras.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.4-tf'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_z-9t1wGSmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjAtY3u-GcL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, GRU, Embedding\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_oVfl0OGxIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mark_start = 'ssss '\n",
        "mark_end = ' eeee'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EurhLBlpLlKz",
        "colab_type": "text"
      },
      "source": [
        "Model çeviri yaparken decoder ile cümle üretecek ve çeviri gerçekleştirecek. Cümle üretmeye başlaması için başlangıç tokeni veriyoruz. Bu işlem için kelime haznesinde bulunmayan bir başlangıç tokeni üretiyoruz. Cümlenin bir noktada sonlanması gerekir bunun içinde bir token daha belirleyeceğiz ve sonlandırma tokeni olacak. Burada boşluklar çok önemli tokenlerin kelimeye yapışmasını istemeyiz. Bu sebeple başlangıç tokeninin sonuna, bitiş tokeninin ise başına boşluk koyuyoruz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxw9VGAZG85a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_serc = []\n",
        "data_dest = []\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONsw7X-9MJgL",
        "colab_type": "text"
      },
      "source": [
        "Şimdi ise encoder ve decoder için iki liste oluşturuyoruz. Encoder'a inglizce cümle decoder'a ise türkçe cümleleri vereceğiz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peraBcnuHUj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for line in open('/content/drive/My Drive/NLP/NLP/5- Makine Çevirisi/tur.txt', encoding = 'UTF-8'):\n",
        "  en_text, tr_text = line.rstrip().split ('\\t')\n",
        "  \n",
        "  tr_text = mark_start + tr_text + mark_end\n",
        "\n",
        "  data_serc.append(en_text)\n",
        "  data_dest.append(tr_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZmg5frXMTmU",
        "colab_type": "text"
      },
      "source": [
        "Artık verileri alıp listeye ekleyebiliriz. Veriler tur.txt dosyasının içerisinde yer alıyor. Veri dosyasında her satırda ingilizce cümleler ve türkçe çevirileri var. For döngüsüyle satır satır geçerek ingilizce cümleyi ve türkçe çevirisini alacağız. Daha sonra döngünün içinde yer alan veriyi ingilizce, türkçe olarak ayırıp iki farklı listeye yazacağız. Encoding utf-8 yapmazsak türkçe karakterlerde sıkıntılar yaşayabiliriz. Veri setinde inglizce ve türkçe cümleler tab ile ayrıldığı için split ile veriyi ikiye ayırıyoruz. Rstrip kodu ise satırdaki gereksiz boşluk karakterlerini silecektir. Şimdi tr_text'e başlangıç ve bitiş tokenini ekliyoruz. Cümleleri listeye eklemeye hazırız. Verinin bazı elemanlarına bakalım."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOKPJ0NWHGwy",
        "colab_type": "code",
        "outputId": "c5148bfa-aa39-4029-9405-920b79027463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "idx = 200\n",
        "data_serc[idx]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How deep?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkwT2q5YHRbz",
        "colab_type": "code",
        "outputId": "60d13b89-fa1a-444d-d539-97f9b65b4949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data_dest[idx]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss Ne kadar derin? eeee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfO4_qwKHk-J",
        "colab_type": "code",
        "outputId": "a0b26498-caf8-4817-f3cf-09c6a0dc2d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(data_serc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "473035"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gT1NT2rHuwO",
        "colab_type": "code",
        "outputId": "7ff9dd18-ced7-417c-ef2a-dda0b4259f6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(data_dest)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "473035"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLoLhMimOjHw",
        "colab_type": "text"
      },
      "source": [
        "Her iki veride de toplamda 473035 tane cümle bulunmaktadır. Şimdi tokenleştirme yapalım. bu işlem için bir sınıf yazacağız. İşimizi kolaylaştırmak için tokenizer'a özellikler ekleyeceğiz. Tokenleştirme için adımlar sınıfta toplanacak bu sayede encoder ve decoder kolay bir şekilde tokenleştirilcek. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv9zwc6LHv9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_words = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mElHBlo7H3Br",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TokenizerWrap(Tokenizer):\n",
        "    def __init__(self, texts, padding,\n",
        "                 reverse=False, num_words=None):\n",
        "       \n",
        "        Tokenizer.__init__(self, num_words=num_words)\n",
        "\n",
        "        self.fit_on_texts(texts)\n",
        "\n",
        "        self.index_to_word = dict(zip(self.word_index.values(),\n",
        "                                      self.word_index.keys()))\n",
        "\n",
        "        self.tokens = self.texts_to_sequences(texts)\n",
        "\n",
        "        if reverse:\n",
        "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
        "            truncating = 'pre'\n",
        "        else:\n",
        "            truncating = 'post'\n",
        "\n",
        "        self.num_tokens = [len(x) for x in self.tokens]\n",
        "        self.max_tokens = np.mean(self.num_tokens) \\\n",
        "                          + 2 * np.std(self.num_tokens)\n",
        "        self.max_tokens = int(self.max_tokens)\n",
        "\n",
        "\n",
        "        self.tokens_padded = pad_sequences(self.tokens,\n",
        "                                           maxlen=self.max_tokens,\n",
        "                                           padding=padding,\n",
        "                                           truncating=truncating)\n",
        "\n",
        "    def token_to_word(self, token):\n",
        "        word = \" \" if token == 0 else self.index_to_word[token]\n",
        "        return word \n",
        "\n",
        "    def tokens_to_string(self, tokens):\n",
        "        words = [self.index_to_word[token]\n",
        "                 for token in tokens\n",
        "                 if token != 0]\n",
        "        text = \" \".join(words)\n",
        "        return text\n",
        "    \n",
        "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
        "        tokens = self.texts_to_sequences([text])\n",
        "        tokens = np.array(tokens)\n",
        "\n",
        "        if reverse:\n",
        "            tokens = np.flip(tokens, axis=1)\n",
        "            truncating = 'pre'\n",
        "        else:\n",
        "            truncating = 'post'\n",
        "\n",
        "        if padding:\n",
        "            tokens = pad_sequences(tokens,\n",
        "                                   maxlen=self.max_tokens,\n",
        "                                   padding='pre',\n",
        "                                   truncating=truncating)\n",
        "\n",
        "        return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sXpXmqnO69S",
        "colab_type": "text"
      },
      "source": [
        "Tokenizerwrap çağırıldığı zaman python bir nesne oluşturur ve ilk nesneyi inite verir. Buraya ilk olarak hep self verilir, nesnenin kendisidir. Daha sonra sınıf içerisine alacağımız parametreler verilir. Önce text ile tokenleştirme işlemi yapacağımız için text'i alıyoruz. Daha sonra padding ile tokenlerin başına mı sonuna mı padding eklenecek belirteceğiz. Son parametremiz olan num_words ile kelime haznesinde kaç kelime olacağını belirtiyoruz. Şimdi tokenleştirme işlemlerini yazalım. Aldığımız texti tokenleştiriyoruz. Word_index ile anahtar ve değerlerin yerini değiştireceğiz. Normalde word_indeks içerisinde anahtar kelimelerden, değerler ise kelimeleri sembolize eden değerlerden oluşuyor. Biz burada sayıları anahtar yapıp değerleri kelimeler yapacağız ve bunu bir sözlükte (dict komutu) kaydedeceğiz.\n",
        " \n",
        "Bu şekilde elimizdeki sayılar hangi kelimeye karşılık geliyor bulabilir, tokenleri tekrar stringe dönüştürebiliriz. Şimdi sözlük için dict komutunu kullanıyoruz ve zip'in içerisine önce anahtarları sonra değerleri veriyoruz. Values içinde kelimelere karşılık gelen sayılar var bu sayıları anahtar yapıyoruz. Daha sonra kelimeleri alıyoruz.\n",
        "\n",
        "Elimizdeki yazıları tokenler olarak bir listede topluyoruz. Tüm inputları aynı boyuta getirmek için padding ekliyoruz. Başa token eklemeye pre padding, sona token eklemeye ise post padding deniyor. Pading yaparken boyut belirlendiğinde boyuttan büyük olanı kısaltıyorduk bu işleme ise trancating deniyor ve bu da padding gibi iki farklı yönteme sahiptir. Pre derken baştan, post derken sondan kısaltılır.\n",
        "\n",
        "Çeviri yaparken hem pre hem post kullanılır. Encoder için pre, decoder için post uygulayacağız. Reverse içerisinde koşula göre trancating true ise pre, false ise post olacak. Cümle başı bizim için önemli olduğu için cümleyi ters çevireceğiz. Tüm kelimelerin yerini değiştireceğiz. Bu işlemi ise reversed ile gerçekleştirdik."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icadCrf5R16a",
        "colab_type": "text"
      },
      "source": [
        "Max_tokens büyüdükçe eğitim hızı düşecektir. Artık cümleyi tokenleştirdik ve tokenleştirme işlemi yaptık. Sınıf sayesinde hızlı bir şekilde encoder ve decoder için tokenleştirme gerçekleştirebiliriz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRyxSiVzJjuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_src = TokenizerWrap(texts=data_serc,\n",
        "                              padding='pre',\n",
        "                              reverse=True,\n",
        "                              num_words=num_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHih6h1QJnn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_dest = TokenizerWrap(texts=data_dest,\n",
        "                               padding = 'post',\n",
        "                               reverse=False,\n",
        "                               num_words=num_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ycso215QSX8V",
        "colab_type": "text"
      },
      "source": [
        "Yukarıdaki iki kod ile ingilizce ve türkçe cümlelerin bulunduğu verileri tokenleştirdik."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUbKnI46L3ww",
        "colab_type": "code",
        "outputId": "a45a3258-a7ac-4a70-886b-681857784d2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "tokens_src = tokenizer_src.tokens_padded\n",
        "tokens_dest = tokenizer_dest.tokens_padded\n",
        "print(tokens_src.shape)\n",
        "print(tokens_dest.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(473035, 11)\n",
            "(473035, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFRBa5mPSsUf",
        "colab_type": "text"
      },
      "source": [
        "Veri sayısı ingilizce(encoder) ve türkçe(decoder) için 473035 geldi.\n",
        "\n",
        "İngilizce kelimelerin boyutu ise 11 olarak belirlenmiş. Bu 11 tane token olacak demektir. Türkçe kelimelerin boyutu ise 10 olarak gelmiştir. Türkçe sondan eklemeli bir dil olduğu için türkçe de ingilizceden daha az kelime vardır. bu yüzden boyut küçük olmuş."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3ZT7EMhMJ4s",
        "colab_type": "code",
        "outputId": "b4ec63c1-5446-4280-fe95-6f123b311d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "token_start = tokenizer_dest.word_index[mark_start.strip()]\n",
        "token_start"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktTBGQGCMW7Q",
        "colab_type": "code",
        "outputId": "8466d87e-69f8-4274-8cdf-fe4fd366b1a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "token_end = tokenizer_dest.word_index[mark_end.strip()]\n",
        "token_end"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-HmJMVqTajY",
        "colab_type": "text"
      },
      "source": [
        "Başlangıç tokeninin 1, bitiş tokeninin ise 2 olduğunu görüyoruz. Şimdi bazı örneklere bakalım."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpd9vcYtMjuH",
        "colab_type": "code",
        "outputId": "a4b0bb60-a57b-4037-ddeb-aeeb8976d0b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "idx = 200\n",
        "tokens_src[idx]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    0,    0,    0,    0,    0,    0,    0,    0, 1368,   42],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltayAA4PXFWT",
        "colab_type": "text"
      },
      "source": [
        "Her bir array'i, ona karşılık gelen kelimeye dönüştürerek orijinal metni yeniden oluşturabiliriz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKF0P3D8Mo_L",
        "colab_type": "code",
        "outputId": "2aaa5e8b-8358-4b39-e699-84a2f3a4588c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer_src.tokens_to_string(tokens_src[idx])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'deep how'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R69NwlCrMucb",
        "colab_type": "code",
        "outputId": "43895a0f-73af-4b4f-9ab9-da852044587a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data_serc[idx]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How deep?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPpPDBOMMxX8",
        "colab_type": "code",
        "outputId": "192ddb59-374b-4a69-e78d-89905776c42b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "tokens_dest[idx]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,   10,   20, 1355,    2,    0,    0,    0,    0,    0],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-v4qcYYXpVm",
        "colab_type": "text"
      },
      "source": [
        "Burada da her bir array'i, ona karşılık gelen kelimeye dönüştürerek türkçe çevirisini görebiliriz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDB7FdUWM2Kj",
        "colab_type": "code",
        "outputId": "23fafbb3-418a-4a6f-f869-c933b8ff0b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer_dest.tokens_to_string(tokens_dest[idx])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss ne kadar derin eeee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAHiXEFcM6x_",
        "colab_type": "code",
        "outputId": "5945bf67-05a3-4443-c536-bff45ed5cbf5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data_dest[idx]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss Ne kadar derin? eeee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCGaSVlCYjLt",
        "colab_type": "text"
      },
      "source": [
        "Tokenleştirme işlemi gerçekleşti şimdi encoder'a vereceğimiz inputu ve decoder'a vereceğimiz input, output'u ayarlayacağız. Outputu başlangıç tokeni olmadan vereceğiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-e8bic9Y00_",
        "colab_type": "text"
      },
      "source": [
        "Verileri sinir ağını eğitmek için kolayca hazırlayabiliriz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68kBEMMlNDNY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input_data = tokens_src"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVThB-KANILO",
        "colab_type": "code",
        "outputId": "60e6b5bf-d8b8-4f86-8937-fc07894c4f08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "decoder_input_data = tokens_dest[:,:-1]\n",
        "decoder_input_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(473035, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNCN5KsTNOJ9",
        "colab_type": "code",
        "outputId": "fc010e3f-05de-4d5b-eb56-f4d6aa08eafe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "decoder_output_data = tokens_dest[:,1:]\n",
        "decoder_output_data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(473035, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEj1AP-_ZGWs",
        "colab_type": "text"
      },
      "source": [
        "Hem input hem output aynı numpy arrayi içerisinde. Burada farklı yansımalarını aldık ve hafızadan yer kazandık."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2zg9n_WNV53",
        "colab_type": "code",
        "outputId": "8e8835d1-e30d-4cd4-ded7-1c27cede6d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "idx=200\n",
        "decoder_input_data[idx]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   1,   10,   20, 1355,    2,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRVjv8hyNeB6",
        "colab_type": "code",
        "outputId": "8cb54a6b-24be-456d-b5cc-f398e3a44fe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "decoder_output_data[idx]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  10,   20, 1355,    2,    0,    0,    0,    0,    0], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxaKVt63Ngl2",
        "colab_type": "code",
        "outputId": "a572ab78-a6cc-4710-f42e-0fbd7a8762ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer_dest.tokens_to_string(decoder_input_data[idx])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ssss ne kadar derin eeee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tZmDh6zNomw",
        "colab_type": "code",
        "outputId": "6d8bea27-72a5-4b6f-9e61-acc5f844f550",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenizer_dest.tokens_to_string(decoder_output_data[idx])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ne kadar derin eeee'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrRyrdvsZr2e",
        "colab_type": "text"
      },
      "source": [
        "İlk önce, bir tamsayı tokenleri dizisini bir \"düşünce vektörüne\" eşleyen sinir ağının kodlayıcı kısmını yaratıyoruz. Bunun için ilk olarak sinir ağının tüm katmanları için nesneleri oluşturduğumuz ve daha sonra bunları bağladığımız Keras'ın işlevsel API'sını kullanacağız, bu Keras'taki sıralı API'den daha fazla esneklik sağlar, daha karmaşık mimarileri denerken ve encoder ile decoderı bağlamanın yolları için faydalıdır.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyCn2A8CNsxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input = Input(shape=(None,), name='encoder_input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efXLxu0DN0gN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_size=128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NC0_nGhQN2ax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_embedding = Embedding(input_dim=num_words,\n",
        "                              output_dim=embedding_size,\n",
        "                              name='encoder_embedding')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-OwuX9FbKG-",
        "colab_type": "text"
      },
      "source": [
        "Model oluşturmaya encoder ile başladık.Keras için embedding layer oluşturuyoruz. ilk önce input_dim ile toplam kelime sayısını söylüyoruz. Output_dim ise vektör uzunluğudur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY-McJ6JN_6F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "state_size = 512 #bütün GRU layerlar 512 boyutlu output üretecek."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBxLwv-_OCAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
        "                   return_sequences=True)\n",
        "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
        "                   return_sequences=True)\n",
        "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
        "                   return_sequences=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hm2NHgvmb1eQ",
        "colab_type": "text"
      },
      "source": [
        "Şimdi layerları bağlamamız gerekiyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFKXhP02ONUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connect_encoder():\n",
        "  net=encoder_input\n",
        "\n",
        "  net=encoder_embedding(net)\n",
        "\n",
        "  net=encoder_gru1(net)\n",
        "  net=encoder_gru2(net)\n",
        "  net=encoder_gru3(net)\n",
        "\n",
        "  encoder_output=net\n",
        "\n",
        "  return encoder_output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciYHA-nXOjK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_output = connect_encoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De2l3BsqcW1j",
        "colab_type": "text"
      },
      "source": [
        "Önce embedding ürettik daha sonra 3 katlı gru kullanarak bir düşünce vektörü ürettik.\n",
        "\n",
        "Şimdi decoder üretelim. Decoder'a başlangıç tokeniyle düşünce vektörünü vereceğiz ve model cümle üretecek. Bu cümlenin encoder'a verilen cümlenin çevirisi olmasını bekliyoruz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xeabVfqOnrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_initial_state = Input(shape=(state_size,),\n",
        "                              name='decoder_initial_state')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiJ-BEQJOzQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_input = Input(shape=(None,), name='decoder_input')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVD0YGTWcfHZ",
        "colab_type": "text"
      },
      "source": [
        "input_dim ile kelime haznemizdeki kelime sayısını belirttik. Output_dim ise vektör uzunluğudur. şimdilik kelime vektörlerini rastgele sayılardan oluşturduk ancak eğitim esnasında bu vektörler optimize edilecek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezVjOHAcO5Vs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_embedding= Embedding(input_dim=num_words,\n",
        "                             output_dim=embedding_size,\n",
        "                             name='decoder_embedding')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFv4JpikPE8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
        "                   return_sequences=True)\n",
        "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
        "                   return_sequences=True)\n",
        "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
        "                   return_sequences=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZNu9x69cw6S",
        "colab_type": "text"
      },
      "source": [
        "Cümle üretmesini istediğimiz için son return_sequences'i true olarak bıraktık. Böylece input, embedding, GRU layerlarını oluşturduk. Decoder'ın çalışması üzerine kelimeler elde etmek istiyoruz. Şu an 512 tane vektör elde ediyoruz ve bunu kelimelere dönüştürmeliyiz. Bunu yapmak için dense oluşturacağız. Gru sonucunu one hot türünde  array'a dönüştüreceğiz. Bu vektörde en büyük değere sahip elemanın indeksi output olarak hangi kelimenin verilmesi gerektiğini belirleyecek."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-_FU1RtPHo-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_dense = Dense(num_words,\n",
        "                      activation='softmax',\n",
        "                      name='decoder_output')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99jwtHbzPP1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def connect_decoder(initial_state):\n",
        "  net=decoder_input\n",
        "\n",
        "  net=decoder_embedding(net)\n",
        "  \n",
        "  net=decoder_gru1(net,initial_state=initial_state)\n",
        "  net=decoder_gru2(net,initial_state=initial_state)\n",
        "  net=decoder_gru3(net,initial_state=initial_state)\n",
        "\n",
        "  decoder_output = decoder_dense(net)\n",
        "\n",
        "  return decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88vrAjdydWOT",
        "colab_type": "text"
      },
      "source": [
        "İnitial_state encoder'ın ürettiği düşünce vektörüdür. Bir önceki layerı parantez içinde vererek layerları bağlamış oluyoruz. Encoder ve decoder'ı birbirine bağlayıp uçtan uca modeli eğitelim. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHMcpf4LKLmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_output = connect_decoder(initial_state=encoder_output)\n",
        "\n",
        "model_train = Model(inputs=[encoder_input,decoder_input],\n",
        "                    outputs=[decoder_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zRlcd7xdfZ8",
        "colab_type": "text"
      },
      "source": [
        "Eğitebileceğimiz bir model oldu. encoder ve decoder'ı birbirine bağlamış bulunuyoruz. İki model daha üreteceğiz (biri encoder diğeri decoder için) bunlar çeviri yapmamızda işimize yarayacak. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCviQRYrKj9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_encoder = Model(inputs=[encoder_input],\n",
        "                      outputs=[encoder_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9EYASF0Kufm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_output = connect_decoder(initial_state = decoder_initial_state)\n",
        "\n",
        "model_decoder = Model(inputs=[decoder_input, decoder_initial_state], \n",
        "                      outputs=[decoder_output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg9Lg2B8dqa4",
        "colab_type": "text"
      },
      "source": [
        "Oluşturduğumuz ilk model ile sinir ağımızı eğiteceğiz diğer iki modelle ise eğitimi tamamladıktan sonra çeviri yapmak için kullanacağız. Gerçek değerlerin bulunduğu one hot array oluşturacağız."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9CRytfCNVWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_train.compile(optimizer = RMSprop(lr=1e-3),\n",
        "                    loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iouOoHLxeFYK",
        "colab_type": "text"
      },
      "source": [
        "Burada yer alan loss fonk ile tahmin edilen değerle gerçekte olması gereken değer karşılaştırılacak ve hatanın büyüklüğünü belirten değer gelecektir. Eğitim yapabilmek için loss değerine ihtiyacımız var.\n",
        "\n",
        "RDD'lerde iyi çalıştığı için optimizer için RMSprop kullanıyoruz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TJsVWewNi1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = \\\n",
        "{\n",
        "    'encoder_input': encoder_input_data,\n",
        "    'decoder_input': decoder_input_data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygY_SAQxNohl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_data = \\\n",
        "{\n",
        "    'decoder_output': decoder_output_data\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-pK7iTaNp7b",
        "colab_type": "code",
        "outputId": "e0092e04-f285-429f-9938-e24229cefcfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "validation_split = 10000 / len(encoder_input_data)\n",
        "validation_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.021140084771739936"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNJDjPtdNrvC",
        "colab_type": "code",
        "outputId": "8faf0d83-d99b-4f56-ed26-51b91526fc88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "model_train.fit(x=x_data,\n",
        "                y=y_data,\n",
        "                batch_size=256,\n",
        "                epochs=1,\n",
        "                validation_split=validation_split)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 463035 samples, validate on 10000 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f689566d378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <function Function._initialize_uninitialized_variables.<locals>.initialize_variables at 0x7f689566d378> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: module 'gast' has no attribute 'Num'\n",
            "463035/463035 [==============================] - 10101s 22ms/sample - loss: 2.3690 - val_loss: 4.8115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f68a36c3518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFh3MeVafFMf",
        "colab_type": "text"
      },
      "source": [
        "Modeli eğitiyoruz ve inputları, outputları veriyoruz. Her iterasyonda 256 veri veriyoruz ve her seferinde o veriyi eğitiyoruz. Verilerin üzerinden 1 defa eğitim gerçekleşecektir. Bu kod çalıştırıldığında model eğitilmeye başlayacaktır. Ekran kartınızın ram'ı daha fazlaysa epochs değerinizi arttırabilirsiniz. Daha iyi sonuçlar alırsınız."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbjLnglJNuBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(input_text, true_output_text=None):\n",
        "  #input için tokenleştirme yapıyoruz.\n",
        "  input_tokens=tokenizer_src.text_to_tokens(text=input_text,\n",
        "                                             reverse=True,\n",
        "                                             padding='pre')\n",
        "  #düşünce vektörü üretiyoruz.\n",
        "  initial_state = model_encoder.predict(input_tokens)\n",
        "  #decoder için max. token sayısı kadar döngü dönecek\n",
        "  max_tokens= tokenizer_dest.max_tokens\n",
        "\n",
        "  shape = (1,max_tokens)\n",
        "  decoder_input_data= np.zeros(shape=shape, dtype=np.int)\n",
        "\n",
        "  token_int=token_start #başlangıç tokeni yani ssss\n",
        "  output_text='' \n",
        "  count_tokens=0 #döngünün kaç defa döndüğünü sayacağız.\n",
        "\n",
        "  while token_int != token_end and count_tokens < max_tokens:\n",
        "    decoder_input_data[0, count_tokens] = token_int\n",
        "    x_data = \\\n",
        "    {\n",
        "        'decoder_initial_state':initial_state,\n",
        "        'decoder_input': decoder_input_data\n",
        "    }\n",
        "\n",
        "    decoder_output = model_decoder.predict(x_data) \n",
        "\n",
        "    token_onehot = decoder_output[0, count_tokens, :] \n",
        "    token_int = np.argmax(token_onehot) #en yüksek indexe sahip elemanı elde ediyoruz.\n",
        "\n",
        "    sampled_word = tokenizer_dest.token_to_word(token_int) #burada ise yüksek indexe sahip kelimeyi yazdırıyoruz.\n",
        "    output_text += \" \" + sampled_word\n",
        "    count_tokens += 1 \n",
        "\n",
        "  output_tokens = decoder_input_data[0] #üretilen cümleyi yazdıralım.\n",
        "\n",
        "  print(\"input text:\")\n",
        "  print(input_text)\n",
        "  print()\n",
        "\n",
        "  print(\"translated text:\")\n",
        "  print(output_text)\n",
        "  print()\n",
        "\n",
        "  if true_output_text is not None:\n",
        "    print(\"true output text:\")\n",
        "    print(true_output_text)\n",
        "    print()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1082OBVfzNd",
        "colab_type": "text"
      },
      "source": [
        "Eğitilmiş modeli kullanarak çeviriyi yapalım. İngilizce cümleyi model encoder'a vereceğiz ve düşünce vektörü elde edeceğiz bunu ve başlangıç tokenini model decoder'a vereceğiz. Tamamen türkçe cümle vermiyoruz böylece modelin cümle üretmesini sağlıyoruz.\n",
        "\n",
        "Modelin çevirisini ve doğru çeviriyi karşılaştırıyoruz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6mf-y3owrPa",
        "colab_type": "code",
        "outputId": "0f2cf85d-f24a-4904-fd08-4a4b1132b278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "translate(input_text=data_serc[10000],\n",
        "          true_output_text=data_dest[10000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input text:\n",
            "I like it, too.\n",
            "\n",
            "translated text:\n",
            " ben de onu seviyorum eeee\n",
            "\n",
            "true output text:\n",
            "ssss Onu ben de beğeniyorum. eeee\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4XfV2MplQZK",
        "colab_type": "code",
        "outputId": "7c766bad-87c7-4c39-bbab-dace086205fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "translate(input_text=data_serc[5000],\n",
        "          true_output_text=data_dest[5000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input text:\n",
            "Tom seems OK.\n",
            "\n",
            "translated text:\n",
            " tom iyi görünüyor eeee\n",
            "\n",
            "true output text:\n",
            "ssss Tom iyi görünüyor. eeee\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcuiSspplTKz",
        "colab_type": "code",
        "outputId": "e7959eb4-14c8-4ef0-d841-c483a4cb6a8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        }
      },
      "source": [
        "translate(input_text=data_serc[70000],\n",
        "          true_output_text=data_dest[70000])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input text:\n",
            "That's totally wrong.\n",
            "\n",
            "translated text:\n",
            " bu tamamen yanlış eeee\n",
            "\n",
            "true output text:\n",
            "ssss Bu tamamen yanlış. eeee\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPER1H5zw0lI",
        "colab_type": "code",
        "outputId": "dad0a51e-7456-45bb-a132-5d0641b416b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "translate(input_text='What is your name?')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input text:\n",
            "What is your name?\n",
            "\n",
            "translated text:\n",
            " adı nedir eeee\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Zif79WncZY",
        "colab_type": "text"
      },
      "source": [
        "Bu cümle veri setinde bulunmuyor. Model cümleyi iyi çevirdi."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmmYjObsoG44",
        "colab_type": "text"
      },
      "source": [
        "Kaynaklar;\n",
        "\n",
        "https://www.tensorflow.org/\n",
        "\n",
        "https://github.com/Hvass-Labs/TensorFlow-Tutorials\n",
        "\n",
        "https://www.youtube.com/results?search_query=neural+machine+translation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-5qlm2lookv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}